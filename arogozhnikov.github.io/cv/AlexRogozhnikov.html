<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>
        CV Alex Rogozhnikov
    </title>
    <style>
        body {
            font-size: 14px;
            /*font-family: sans-serif;*/
        }

        body {
            max-width: 800px;
            margin: auto;
        }

        .demonstrations img {
            float: left;
            width: 130px;
            height: 130px;
            margin: 5px;
            margin-right: 10px;
            border-radius: 75px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.6);
        }

        .demonstrations {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
        }

        .demonstrations>div {
            width: 45%;
        }
        .pagebreak { page-break-before: always; } 

    </style>
</head>

<body>
    <!-- <div style="width: 110px; height: 110px; border-radius: 100px; box-shadow: 0px 0px 5px 0 #000; float: left;
            margin: 25px; display: inline-block;
              background-position: center center;
              background-size: cover;
              background-image: url('images/alex_rogozhnikov.jpeg');">
    </div> -->
    <!--<img src='images/alex_rogozhnikov.jpeg' style='height: 400px; width: 400px;' />-->
    <h1>Alex Rogozhnikov</h1>
    <!--russian: Алексей Михайлович Рогожников-->
    <!-- <h3>Research scientist</h3> -->
    <p>
        <!-- Moscow, Russia <br /> -->
        Email: <a href='mailto:alex.rogozhnikov@yandex.ru'>alex.rogozhnikov@yandex.ru</a> <br /> Blog: <a href='https://arogozhnikov.github.io'>Brilliantly wrong</a><br
        /> Github: <a href='https://github.com/arogozhnikov'>https://github.com/arogozhnikov</a> <br /> ResearchGate:
        <a
            href='https://www.researchgate.net/profile/Alex_Rogozhnikov'>RG/Alex_Rogozhnikov</a> <br />
    </p>


    <h2>Education and Career</h2>
    <p>
        2014-2017 &mdash; research scientist at Yandex (leading search engine in Russia), 
             worked on joint research projects with CERN (European Organization for Nuclear Research). <br />
        2015 &mdash; Ph.D. in computer science from Moscow State University <br /> 
        2014 &mdash; M.Sc. in machine learning from Yandex School of Data Analysis <br /> 
        2014 &mdash; M.Sc. in mathematical physics from Higher School of Economics (diploma with honors) <br /> 
        2012 &mdash; M.Sc. in computer science from Moscow State University (diploma with honors) <br /> 
    </p>
    <p>
        Previously a member of CERN experiments: LHCb, SHiP, and an associated member of the OPERA experiment at INFN. <br />
        Winner of national olympiads in physics and mathematics.
        <!-- In different years attended summer schools on matrix methods, statistical physics and information retrieval. 
        In 2007-2009 I attended evening classes on theoretical physics at Lebedev Physics Institute. -->
    </p>


    <h2>Research Experience</h2>
    <p>
        In my recent research, I applied machine learning to different problems in high energy physics at the Large Hadron Collider (and some other experiments).
        I have developed machine learning-based approaches for particle identification, tracking, online data filtering, flavour tagging and particle shower detection.
        Proposed three machine learning algorithms to address issues specific to HEP applications. More details are given in 'Research Projects' section.
    </p>
    <p>
        Previous research topics include optimal control, mathematical physics, numerical methods and solid state theory.    
    </p>

    <h2>Selected publications </h2>
    <ul>
        <li>
            Rogozhnikov, Alex, and Tatiana Likhomanenko. "InfiniteBoost: building infinite ensembles with gradient descent." arXiv preprint arXiv:1706.01109 (2017).
        </li>
        <li>
            Likhomanenko, T., Derkach, D., & Rogozhnikov, A. (2016). Inclusive Flavour Tagging Algorithm. In Journal of Physics: Conference Series (Vol. 762, No. 1, p. 012045). IOP Publishing.
        </li>
        <li>
            Rogozhnikov, A. (2016). Reweighting with boosted decision trees. In Journal of Physics: Conference Series (Vol. 762, No. 1, p. 012036). IOP Publishing.        
        </li>
        <li>
            Rogozhnikov, A., Bukva, A., Gligorov, V., Ustyuzhanin, A., & Williams, M. (2015). New approaches for boosting to uniformity. Journal of Instrumentation, 10(03), T03002.
        </li>
        <li>
            Likhomanenko, T., Ilten, P., Khairullin, E., Rogozhnikov, A., Ustyuzhanin, A., & Williams, M. (2015). LHCb Topological Trigger Reoptimization. In Journal of Physics: Conference Series (Vol. 664, No. 8, p. 082025). IOP Publishing.
        </li>
        <li>
            Likhomanenko, T., Rogozhnikov, A., Baranov, A., Khairullin, E., & Ustyuzhanin, A. (2015). Reproducible Experiment Platform. In Journal of Physics: Conference Series (Vol. 664, No. 5, p. 052022). IOP Publishing.
        </li>
        <li>
            Alcaraz, F. C., Brankov, J. G., Priezzhev, V. B., Rittenberg, V., & Rogozhnikov, A. M. (2014). Noncontractible loops in the dense O (n) loop model on the cylinder. Physical Review E, 90(5), 052138.
        </li>
        <li>
            Rogozhnikov, A. M. (2013). Optimal control of longitudinal vibrations of composite rods with the same wave propagation time in each part. Differential Equations, 49(5), 607-616.
        </li>
        <li>
            Rogozhnikov, A. M. (2012). Study of a mixed problem describing the oscillations of a rod consisting of several segments with arbitrary lengths. In Doklady Mathematics (Vol. 85, No. 3, pp. 399-402). SP MAIK Nauka/Interperiodica.
        </li>

    </ul>
    <h2>Conferences</h2>
    <p> 
        Talks at conferences and workshops: ACAT 2016 (Chile), 
        Heavy Flavour Data Mining Workshop 2016 (Zurich), 
        International Workshop on Nuclear Emulsions 2016 (Naples), 
        IML Machine Learning workshop 2017 (Geneva).
        Co-authors also presented results at CHEP 2015 (Okinawa, Japan), ML prospects and applications 2015 (Berlin) and ACAT 2017 (upcoming, Seattle, USA).   
    </p>


    <div class='pagebreak'></div>



    <h2>Software Development</h2>

    <p>
        I am a developer of <a href='https://github.com/arogozhnikov/hep_ml'>hep_ml</a> package of machine learning algorithms
        for high energy physics with sklearn-compatible interface and previously one of core contributors to <a href='https://github.com/yandex/REP'>yandex/REP</a>,
        a docker-based environment for data analysis in particle physics with jupyter + python + ROOT.
    </p>
    <p>
        In research projects I am doing everything in python (and use modern fortran to optimize critical places). 
        Know theano and pytorch, familiar with keras, lasagne and tensorflow.  
        
        Using javascript from time to time, with glsl / hlsl for visualizations.
        <br /> Previously used C# and .NET platform, had some experience in C++, PHP, SQL. Had courses in MATLAB, assembler;
        passed the CUDA certification.
    </p>
    <p>
        In 2012, our team got <strong>1st place</strong> (out of more than 500) in international competition "Accelerate Your Code" by Intel, we provided
        the fastest parallel DNA processing system in C++ with openmp.
    </p>



    <h2>Teaching</h2>
    <p>
        <!-- Materials of the courses are available on github, corresponding links are provided: -->
        <!-- Links are github repositories with courses materials -->
    </p>
    <ul>
        <li>
            Course of machine learning at Imperial College, London 
            (<a href='https://github.com/arogozhnikov/YSDA_ICL'>2015</a>,
            <a href='https://github.com/yandexdataschool/MLatImperial2016'>2016</a>,
            <a href='https://github.com/yandexdataschool/MLatImperial2017'>2017</a>) &mdash; practical classes + assignments
        </li>
        <li>
            Machine Learning in High Energy Physics school 
            (<a href='https://github.com/yandexdataschool/mlhep2015'>2015: St. Petersburg, Russia</a>;
            <a href='https://github.com/yandexdataschool/mlhep2016'>2016: Lund, Sweden</a>) &mdash; lecturer of main
            track
        </li>
        <li>
            Machine Learning in Science and Industry &mdash; invited lecturer at Heidelberg university
            (<a href='https://github.com/yandexdataschool/MLAtGradDays'>2017, Heidelberg, Germany</a>)
        </li>
    </ul>

    <p>
        I've organized and co-organized around a dozen of in-class data challenges based on kaggle platform. 
        Also, I helped by providing
        and implementing specific evaluation metrics for "<a href='https://www.kaggle.com/c/flavours-of-physics'>Flavours of Physics</a>" challenge at Kaggle run by Yandex &amp; CERN.
    </p>
    <p>
        I visualize machine learning techniques and approaches in my blog called "Brilliantly wrong" ( > 75'000 visitors a year). 
        I'm using these visualizations in my courses.
    </p>






    <!-- 
    <h3>Blog (Brillianly wrong)</h3>
    <p>
        In 2016 I've started posting visual explanations for some topics of machine learning I find interesting. 
        Currently my blog has around 75000 visitors a year. 
    </p> -->
    <!--
    <div class='demonstrations'>
        <div>
            <a href="https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo">
                <img src="images/mini_hmc_explained.png" alt="">
                    Hamiltonian Monte Carlo explained
                </a>
            <p>
                Explanation of basic MCMC concepts followed by details about Hamiltonian MC
            </p>
        </div>
        <div>
            <a href="https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained">
                <img src="images/mini_gb_explained.png" alt="">
                    Gradient Boosting explained
                </a>
            <p>
                Interactive explanation of gradient boosting for regression
            </p>
        </div>
        <div>
            <a href="https://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground">
                <img src="images/mini_gb_playground.png" alt="">
                Gradient boosting playground
                </a>
            <p>
                The right place to understand important knobs of gradient boosting
            </p>
        </div>
        <div>
            <a href="https://arogozhnikov.github.io/3d_nn">
                <img src="images/mini_3d_nn.png" alt="">
                Neural Networks visualized in 3d
                </a>
            <p>
                Testing a fancy way of visualizing simple neural network with shaders in WebGL
            </p>
        </div>
    </div>
    <p>
        I use these (and other, more simple and less detailed) demonstrations in teaching.
    </p>
    -->
    <!--
    <h2>Proposed projects</h2>
    <p>
        I find the following topics both important and interesting for investigation and are fine as a strating point (but I'm completely open to better ideas):
    </p>
    <h3>Faster training of NNs</h3>
    <p>
        There are many algorithms that were proposed over the last decades to train neural networks. Current obvious mainstream is adaptive
        SGD algorithms (Adagrad, RMSProp, Adadelta, Adam, etc.), and it seems that DL community strongly prefers Adam to
        other options.
    </p>
    <p>
        However, some of recent research (see YellowFin) suggests that adaptive optimization may not be required
        (moreover, NLP community prefers non-adaptive SGD). Thus this fundamental topic seems to be under-investigated.
    </p>
    <p>
        Development of more efficient algorithms that require less tuning should reduce costs of training neural networks and
        simplify training without clusters (which is quite important for open research).
    </p>

    <h3>
        Tuning of non-differentiable simulators in an adversarial manner
    </h3>
    <p>
        There are different simulators of real-world processes: graphics rendering, physics simulators (gym has such environments), 
        in particle Monte Carlo simulation of collisions is used.
    </p>
    <p>
        These simulations have many parameters and tuned to achieve better "alignment" with real life. Pictures, motion should look 'realistic'.
        This tuning is usually done manually (by looking at pictures / distributions), while we can automate the
        process of comparison by looking at this in an 'adversarial' manner: good simulation is a simulation that can't be
        discriminated from real processes by machine learning methods.
    </p>
    <p>
        The good question for research now is finding the way to use this criterion efficiently to find best parameters (avoid checking all possible parameters).
        Compared with adversarial networks, generator here is typically not differentiable.
    </p>
    -->

    <br />
    <br />

    <h2>Research projects </h2>

    <h3>InfiniteBoost: building infinite ensembles with gradient descent (with T.Likhomanenko)</h3>
    <p>
        InfiniteBoost is a modification of gradient boosting that converges when a number of trees in the ensemble tends to infinity. 
        In this approach, it is also possible to introduce automated tuning of capacity (it is a parameter that is similar to learning rate in gradient boosting).
        <a href='https://arxiv.org/abs/1706.01109'>Read more</a>
    </p>

    <h3>Particle identification at the LHCb (with D.Derkach, M.Hushchyn, T.Likhomanenko)</h3>
    <p>
        LHCb is one of four major experiments at the LHC, and it is a bit different from other experiments &mdash; LHCb is single arm, and analyzes particles in quite limited
        angle. However, the advantage of this scheme compared to other experiments, is that LHCb provides more information
        to identify particles, which makes it more precise in studies of b-physics.
    </p>
    <p>
        We prepared a major update of particle identification system with deep networks and GBDT. Important part was preparing
        models which are independent on momentum using an approach from "Boosting to uniformity" (see below).
        <a href='https://indico.cern.ch/event/589985/contributions/2430716/attachments/1398000/2131978/PID_presentation.pdf'>Read more</a>
    </p>


    <h3>Finding electromagnetic showers in the OPERA </h3>
    <p>
        The OPERA is an experiment placed inside a mountain in Italy, it was created to confirm neutrino oscillations (Nobel prize
        2015). In this project I've created a system that detects electromagnetic showers in the data collected by the OPERA.
        That is, among millions of base tracks it finds a pattern of several hundred base tracks.
        <a href='https://arogozhnikov.github.io/2017/06/24/opera.html'>Read more</a>
    </p>

    <h3>Reweighting with Boosted Decision Trees</h3>
    <p>
        An important problem of many analyses in high energy physics is a discrepancy between simulated data and real data. 
        An approach used previously to reduce this effect can only handle discrepancies in 1-2 variables.
    </p>
    <p>
        I've proposed an algorithm that directly solves reweighting problem in many dimensions and additionally addresses some issues
        important for LHCb analyses, such as handling negative weights (so-called sWeights). This tool is used in LHCb analyses.
        <a href='http://iopscience.iop.org/article/10.1088/1742-6596/762/1/012036/pdf'>Read more </a>
    </p>

    <!-- <div class='pagebreak'></div> -->


    <h3>
        Inclusive flavour tagging at the LHCb (with D.Derkach, T.Likhomanenko)
    </h3>
    <p>
        Guessing flavour of a neutral (non-charged) meson isn't easy, but required to estimate some of the standard model parameters.
        This information can be partially reconstructed by analyzing tracks left by other particles produced after collision.
    </p>
    <p>
        We came up with a simple probabilistic model which combines information from all the other tracks &mdash; and it works better
        than previous approaches, where separate analysis was performed for each type of tagging particles and for each meson.
        <a href='http://iopscience.iop.org/article/10.1088/1742-6596/762/1/012045/pdf'>Read more</a>
    </p>
    <p>
        Later I've tried to improve the system by including attention-like mechanism. 
        This helps when amount of training data is limited.
        <a href='https://indico.cern.ch/event/595059/contributions/2497373/attachments/1431033/2198204/Rogozhnikov_InclusiveFlavourTagging.pdf'> Read more</a>
    </p>


    <h3>Boosting to uniformity (main author, with A. Bukva, V. Gligorov, A. Ustyuzhanin, and M. Williams)</h3>
    <p>
        Various statistical dependencies may be easy to use to improve classification, but are undesirable to influence our decisions (simplified
        examples are gender and race when using ML in hiring). 
        Simply removing these features from training may not be sufficient, since
        other features may still have this information (e.g. photo in CV helps easily guess the gender, in some languages gender
        of a writer can be inferred from the text).
    </p>
    <p>
        We developed a method that is capable of suppressing dependency between classification result and one or more selected variables
        using specific loss (that is based on Cramer-von Mises criterion). 
        <!-- Method is targeted at applications in high energy physics. -->
        <a href='http://iopscience.iop.org/article/10.1088/1748-0221/10/03/T03002/pdf'>Read more</a>
    </p>

    <h3>Tracking in the COMET (with E. Gillies)</h3>
    <p>
        The COMET is an experiment in high energy physics currently under construction in Japan targeted at finding charged LFV transitions. 
        The goal was to prepare a fast system that efficiently selects candidate events for transitions.
    </p>
    <p>
        Using machine learning coupled with a soft modification of Hough transform we were able to improve wire-level recognition quality:
        ROC AUC from 0.95 to 0.9993.

        <a href='https://indico.shef.ac.uk/indico/event/1/session/4/contribution/45/material/slides/0.pdf'>
                Read more
            </a>
    </p>

    <h3>Inclusive trigger for the LHCb (contributing author) </h3>
    <p>
        Millions of collisions should be analyzed each second at the LHCb experiment, which
        is an enormous amount of data (that can't even be stored), so the experiment uses online triggers that decide which collisions
        to store and which can be deleted.
    </p>
    <p>
        Our team developed new trigger system based on MatrixNet (Yandex proprietary GBDT modification). 
        I was responsible for speeding up the model and managed to compress
        an already trained MatrixNet ensemble from 10'000 trees to 100 without significant drop in quality. 
        <a href='http://iopscience.iop.org/article/10.1088/1742-6596/664/8/082025/meta'>Read more</a>
    </p>



    <h3>Optimal boundary control of oscillations in distributed systems (PhD thesis)</h3>
    <p>
        I was in a group led by Vladimir Il'in (<a href='https://ru.wikipedia.org/wiki/Ильин,_Владимир_Александрович'>Russian wiki</a>)        
        and investigated  the problems of optimal boundary control of oscillations described by wave equation (exciting / damping
        of particular oscillations by actively interacting with a system at the boundary). 
        Typical approaches investigate numerical algorithms to find an approximate solution, 
        our group developed methods to solve the problem analytically, hence precisely.
    </p>
    <p>
        I've introduced a special notation based on operator matrices to describe the problem 
        and provided optimal solution of control problem for composite rods/strings 
        of multiple parts (previous results covered only very specific case with two parts and additional strong requirements).
    </p>
    <p>
        This research was selected as <strong>"best student's paper in mathematics"</strong> by Russian Academy of Sciences in 2012.
    </p>



    <h3>Computing properties of dense loop model using duality with spanning web model (master thesis) </h3>
    <p>
        This is a research in solid state theory: both dense loop model and spanning web model are lattice models (and have corresponding
        partition functions), their nice duality made it possible to compute partition function and loops density of dense
        loop model.
        <a href='https://arxiv.org/pdf/1409.7848.pdf'>Read more</a> (my part is computations for web models).
    </p>

</body>

</html>